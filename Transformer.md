## Transformer란?

Transformer는 **문장의 순서와 의미를 동시에 이해할 수 있는 구조**로,  
기존의 RNN, LSTM을 대체하는 딥러닝 언어 모델 구조입니다.

### 🔑 핵심 개념

- **Self-Attention (자기 주의 메커니즘)**  
  → 문장 내 단어들이 서로 어떤 관계인지 가중치를 계산해 반영

- **포지셔널 인코딩**  
  → 순서를 고려하지 않는 구조를 보완하기 위해 **단어 위치 정보**를 벡터에 추가

---

## 임베딩(Embedding) 모델

임베딩은 **단어를 숫자 벡터로 바꾸는 과정**입니다.

> 비슷한 의미의 단어들은 **비슷한 위치의 벡터**로 표현됩니다.

### 예시

| 문장                            | 의미            | 기대 임베딩              |
| ------------------------------- | --------------- | ------------------------ |
| "나는 **파이썬**을 배운다."     | 프로그래밍 언어 | 📍벡터 A                 |
| "동물원에서 **파이썬**을 봤다." | 뱀              | 📍벡터 B (A와 멀어야 함) |

### 임베딩 방식 종류

| 종류                   | 설명                                                 |
| ---------------------- | ---------------------------------------------------- |
| **Word2Vec, FastText** | 고정 벡터 (단어마다 하나)                            |
| **BERT, BGE 등**       | 문맥 기반 임베딩 (같은 단어도 문장에 따라 다른 벡터) |

---

## 임베딩 학습 방식 비교

| 항목      | MLM (Masked Language Modeling) | Contrastive Learning (문장 유사도) |
| --------- | ------------------------------ | ---------------------------------- |
| 학습 목표 | 문장 내 특정 단어 예측         | 문장 간 의미적 유사성 유지         |
| 예시 모델 | BERT, RoBERTa, KoBERT          | SBERT, BGE, GTE                    |
| 출력      | 토큰 임베딩                    | 문장 임베딩                        |
| 장점      | 세밀한 언어 정보 반영          | 빠르고 의미 중심 임베딩            |
| 단점      | 문장 전체 의미 표현에 약함     | 세부 구조 이해는 부족할 수 있음    |

---

## 사전학습된 LLM은 어떻게 동작할까?

LLM은 사실 **“다음 단어를 맞추는 거대한 분류기”**입니다.

### 🔁 작동 방식 예시

| 입력                       | 예측     |
| -------------------------- | -------- |
| ["나는"]                   | "밥을"   |
| ["나는", "밥을"]           | "먹었다" |
| ["나는", "밥을", "먹었다"] | "."      |

- 단어가 아니라 **토큰** 단위로 예측하며,
- 학습이 반복되면서 **문법과 의미, 흐름**을 모두 배우게 됩니다.

---

## 파인튜닝 (Fine Tuning) vs 인스트럭션 튜닝

| 모델 유형         | 설명                                              |
| ----------------- | ------------------------------------------------- |
| Pretrained        | 지식은 많지만 질문에 반응은 못함 (책만 읽은 사람) |
| Instruction Tuned | 질문에 적절히 반응함 (질문-답변 훈련을 받은 사람) |

### 예시

- 입력: `"나는 밥을 먹고 산책을 했다. 요약해줘."`  
  → **Pretrained 모델**: 적절한 응답이 어려움

- 입력: `"나는 밥을 먹고 산책을 했다. 한 줄 요약은"`  
  → **Instruction Tuned 모델**: `"밥을 먹고 산책했다."`와 같이 자연스러운 응답 생성

> 실제 서비스형 GPT는 대부분 **Instruction 튜닝된 모델**입니다.

---

[파인 튜닝 보기](FineTune.md)
