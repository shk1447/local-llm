# Gemma Fine-tuning 요약

| 모델                     | fine-tune 여부 | 비고                                         |
| ------------------------ | -------------- | -------------------------------------------- |
| BERT / RoBERTa           | ✅ 가능        | 전통적인 encoder 기반                        |
| GPT-2 / GPT-J / GPT-NeoX | ✅ 가능        | decoder 기반, causal LM                      |
| LLaMA / Mistral / Gemma  | ✅ 가능        | QLoRA 등으로 경량 튜닝                       |
| Falcon / Baichuan / Yi   | ✅ 가능        | 일부 라이선스 조건 확인 필요                 |
| GPT-3.5 / GPT-4          | ❌ 불가능      | API 사용만 가능 (일부 제한된 fine-tune 허용) |

## 1. 환경 설정

- 필요한 라이브러리 설치 (`transformers`, `datasets`, `peft`, `bitsandbytes` 등)
- GPU 환경에서 실행 권장

## 2. 데이터 준비

- 번역 데이터셋 로드 (예: 영어-한국어 병렬 데이터)
- 데이터셋 전처리: 입력(영어), 출력(한국어) 형식으로 정제

## 3. 모델 및 토크나이저 불러오기

- `transformers` 라이브러리로 Gemma 모델과 토크나이저 로드
- 사전학습된 모델 사용

## 4. PEFT(LoRA) 설정

- 파인튜닝 효율을 위한 LoRA 적용
- LoRA 설정값(랭크, 알파 등) 지정

## 5. 데이터셋 토크나이즈

- 입력/출력 텍스트를 토크나이즈하여 모델 입력 형태로 변환

## 6. Trainer 설정 및 파인튜닝

- `Seq2SeqTrainer` 또는 `Trainer`로 파인튜닝 설정
- 하이퍼파라미터(에폭, 배치사이즈, 러닝레이트 등) 지정
- 모델 학습 진행

## 7. 평가 및 추론

- 검증 데이터로 번역 품질 평가
- 예시 문장 번역 결과 출력

## 8. 모델 저장 및 활용

- 파인튜닝된 모델 저장
- 추론용 코드로 번역 결과 확인

## 9. 기대되는 결과

- 영어 문장을 입력하면 한국어로 자연스럽게 번역하는 모델 생성
- 기존 사전학습 모델 대비 도메인 특화 또는 품질 향상된 번역 결과 기대
- 실제 서비스 또는 연구에 활용 가능한 커스텀 번역 모델 확보

---

**참고:**  
코드 예시, 하이퍼파라미터, 데이터셋 경로 등은 실제 노트북 파일을 참고하여 구체적으로 작성 필요.

[모델 추론 해보기](Inference.md)
